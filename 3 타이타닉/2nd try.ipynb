{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('타이타닉 data/train.csv')\n",
    "test = pd.read_csv('타이타닉 data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# 1. Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 2. Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# 3. Support vector machine\n",
    "from sklearn.svm import SVC\n",
    "# 4. Gaussian naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# 5. K nearest neighbor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 6. Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# 7. Gradient boosing\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# 8. Neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier # decision tree 앙상블 모델, 부스팅\n",
    "# [light gbm 장점, 하이퍼파라미터 튜닝 방안](https://ariz1623.tistory.com/209)\n",
    "# LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능.\n",
    "# [XGBoost와 LightGBM 하이퍼파라미터 튜닝 가이드](https://psystat.tistory.com/131)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# 모델 평가 지표 scoring metrics\n",
    "from sklearn.model_selection import cross_val_score # model 검증\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "import ast # convert string to function\n",
    "\n",
    "# [scikit-learn classifier metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "# [classification metrics 1](https://hong-yp-ml-records.tistory.com/29)\n",
    "# [classification metrics 2](https://sw-data.tistory.com/23) -> regression metrics 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
       "       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.mean(axis = 0)\n",
    "# train.fillna(train.mean(), axis = 0)\n",
    "train = train.fillna(train.mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B96 B98        4\n",
       "C23 C25 C27    4\n",
       "G6             4\n",
       "E101           3\n",
       "F33            3\n",
       "              ..\n",
       "B50            1\n",
       "C50            1\n",
       "A19            1\n",
       "D15            1\n",
       "D45            1\n",
       "Name: Cabin, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Cabin'].value_counts()\n",
    "train['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Embarked'] = train['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin'] = train['Cabin'].fillna('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Cabin          0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      577\n",
       "female    314\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sex ratio\n",
    "train['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex to int\n",
    "train['Sex'] = train['Sex'].map({'male':0, 'female':1})\n",
    "test['Sex'] = test['Sex'].map({'male':0, 'female':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked to int\n",
    "train['Embarked'] = train['Embarked'].map({'S':1, 'C':2, 'Q':3})\n",
    "test['Embarked'] = test['Embarked'].map({'S':1, 'C':2, 'Q':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Embarked'].unique()\n",
    "# train['Ticket'].unique()\n",
    "# train['Cabin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    int64  \n",
      " 5   Age          891 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        891 non-null    object \n",
      " 11  Embarked     891 non-null    object \n",
      "dtypes: float64(2), int64(6), object(4)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']] # 'Ticket', 'Cabin'\n",
    "y_train = train[['Survived']]\n",
    "X_test = test[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "user_seed = 0\n",
    "random.seed(user_seed) # seed 고정\n",
    "LGBM = lgb.LGBMClassifier(random_state = user_seed)\n",
    "\n",
    "# model = LGBM.fit(X_train, y_train,\n",
    "#                             eval_set=[(X_test, y_test)],\n",
    "#                             eval_metric='auc',\n",
    "#                             early_stopping_rounds=5) # map: mean average precision\n",
    "model = LGBM.fit(X_train, y_train)\n",
    "# eval_metric = logloss, auc, error\n",
    "y_pred_test = model.predict(X_test, num_iteration=LGBM.best_iteration_) # 예측\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred_test)\n",
    "# roc_auc = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "## 분류는 input으로 수치형만 가능한가봄...\n",
    "# DataFrame.dtypes for data must be int, float or bool.\n",
    "# Did not expect the data types in the following fields: Sex, Ticket, Cabin, Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "findata = pd.concat([test['PassengerId'], pd.DataFrame(y_pred_test)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "findata.columns = ['PassengerId', 'Survived']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         1\n",
       "4            896         0\n",
       "..           ...       ...\n",
       "413         1305         1\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         1\n",
       "417         1309         1\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "findata.to_csv('2nd_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "def my_classifier():\n",
    "    \n",
    "    # X, y\n",
    "    X = d1221.iloc[:,2:-1] # 1221 데이터\n",
    "    y = d1221.iloc[:,-1]\n",
    "    \n",
    "    # seed 고정\n",
    "    user_seed = 0\n",
    "    random.seed(user_seed) # seed 고정\n",
    "    \n",
    "    # 모델\n",
    "    # (같은 줄에 써놓은 애들끼리 하이퍼파리미터 구성 비슷하거나 같음)\n",
    "    Logistic_Regression = LogisticRegression(n_jobs=64,\n",
    "                                             random_state=user_seed,\n",
    "                                             max_iter=10) # , verbose = 1\n",
    "    # [모델훈련](https://taek98.tistory.com/15)\n",
    "\n",
    "    DecisionTree = DecisionTreeClassifier(max_depth = 10,\n",
    "                                          random_state=user_seed,\n",
    "                                          max_leaf_nodes=4)\n",
    "    \n",
    "    RandomForest = RandomForestClassifier(n_estimators=10,\n",
    "                                          max_depth=10,\n",
    "                                          random_state=user_seed,\n",
    "                                          max_leaf_nodes=4,\n",
    "                                          n_jobs=64) # , verbose=1\n",
    "    \n",
    "    GradientBoosting = GradientBoostingClassifier(n_estimators=10,\n",
    "                                                  max_depth = 10,\n",
    "                                                  random_state = user_seed,\n",
    "                                                  max_leaf_nodes=4,\n",
    "                                                  learning_rate=0.05) # , verbose = 1\n",
    "    \n",
    "    SVM = SVC(max_iter=1000,\n",
    "              random_state=user_seed) # , verbose=True\n",
    "    \n",
    "    Gaussian_NB = GaussianNB()\n",
    "    KNeighbors = KNeighborsClassifier(n_jobs = 64)\n",
    "    MLP = MLPClassifier(max_iter=1000,\n",
    "                        batch_size = 10000,\n",
    "                        shuffle=True,\n",
    "                        random_state = user_seed,\n",
    "                        early_stopping=True) # early_stopping 있어서 max_iter 키움., verbose=True\n",
    "\n",
    "    XGBoost = XGBClassifier(n_jobs=64,\n",
    "                            max_depth=10,\n",
    "                            n_estimators=10,\n",
    "                            learning_rate=0.05,\n",
    "                            random_state = user_seed) # early_stopping_rounds = 50,\n",
    "    # [XGBoost Classifier hyper params](https://xgboost.readthedocs.io/en/stable/python/python_api.html)\n",
    "    LGBM = lgb.LGBMClassifier(num_leaves=15,\n",
    "                              learning_rate=0.05,\n",
    "                              n_estimators=10,\n",
    "                              max_depth = 10,\n",
    "                              random_state = user_seed) # early_stopping_rounds, best_iteration_,\n",
    "    # max_depth : 과적합 방지를 위해 깊이 크기 제한\n",
    "    # n_estimators : 너무 크면 과적합, 성능저하\n",
    "    # learning_rate 작게 하면서 n_estimators를 크게 하는 것은 부스팅 계열 튜닝에서 가장 기본적인 튜닝 방안이므로 이를 적용하는 것도 좋다.\n",
    "    # [LGBM](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)\n",
    "    \n",
    "    # [gbm grid CV, best_iteration_ 참고](https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/sklearn_example.py)\n",
    "    # [how to save model to best iteration if early stopping does not happen?](https://github.com/Microsoft/LightGBM/issues/1683)\n",
    "    \n",
    "    dummy = DummyClassifier(strategy = 'stratified', random_state = user_seed) # strategy 옵션: 'stratified', 'uniform', 'most_frequent'\n",
    "    \n",
    "    my_model_list = ['MLP', 'DecisionTree', 'RandomForest', 'GradientBoosting',\n",
    "                     'XGBoost', 'dummy', 'Logistic_Regression', 'SVM', 'Gaussian_NB'] # 실수로 빼먹ㅇ서 다시\n",
    "    # 'LGBM', 'KNeighbors' 서버 자꾸 터져서 뺍니다\n",
    "    # , 끝나서 뺍니다.\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    ######################################################################### fitting\n",
    "    for model_nm in tqdm(my_model_list):\n",
    "        print(f'<<<---{model_nm} start--->>>')\n",
    "        print(datetime.now().strftime('%H:%M:%S'))\n",
    "        cntcnt = 0\n",
    "        # cv # 년도 나눠지는 것에 따라서 개수 정함.\n",
    "        for slicer in slicing_dic:\n",
    "            print(f'<<<---{slicer} start--->>>')\n",
    "            print(datetime.now().strftime('%H:%M:%S'))\n",
    "            tmpX = X[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # X를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "            tmpy = y[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # y를 연속 4년치 꺼내서 reset_index(drop = True)\n",
    "#             tmp_all = d1221[slicing_dic[slicer][0]:slicing_dic[slicer][1]].reset_index(drop = True) # 전체데이터\n",
    "\n",
    "            X_train = tmpX[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "            X_test = tmpX[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "            y_train = tmpy[:-slicing_dic[slicer][2]] # 기준년도 앞\n",
    "            y_test = tmpy[-slicing_dic[slicer][2]:] # 기준년도 이하\n",
    "            \n",
    "            # train끼리 idx 같아야 하므로\n",
    "            len_train = slicing_dic[slicer][1]-(slicing_dic[slicer][2]+slicing_dic[slicer][0]) # 혹은 tmp_train.shape[0]\n",
    "            len(list(range(len_train)))\n",
    "            train_idx = list(range(len_train))\n",
    "            random.shuffle(train_idx) # 자동으로 덮어쓰기\n",
    "            \n",
    "            # test끼리 idx 같아야 하므로\n",
    "            len_train_to_test = slicing_dic[slicer][1]-slicing_dic[slicer][0]\n",
    "            test_idx = list(range(len_train,len_train_to_test)) # +1 안해줘도 되는 거 위 cell에서 확인\n",
    "            len(test_idx)\n",
    "            random.shuffle(test_idx) # 자동으로 덮어쓰기\n",
    "            \n",
    "            X_train = X_train.loc[train_idx]\n",
    "            y_train = y_train.loc[train_idx]\n",
    "            X_test = X_test.loc[test_idx]\n",
    "            y_test = y_test.loc[test_idx]\n",
    "            ###########################################\n",
    "            y_train = np.array(y_train).reshape(-1,1) # 1열짜리로 만드는 것\n",
    "            y_test = np.array(y_test).reshape(-1,1)\n",
    "\n",
    "            ######################################################## cv를 위한 데이터 slicing 작업 끝\n",
    "\n",
    "            # fitting은 cv 안에 넣기\n",
    "            if model_nm == 'LGBM':\n",
    "                model_ = eval(model_nm).fit(X_train, y_train,\n",
    "                                            eval_set=[(X_test, y_test)],\n",
    "                                            eval_metric='auc',\n",
    "                                            early_stopping_rounds=5) # map: mean average precision\n",
    "                # eval_metric = logloss, auc, error\n",
    "                y_pred_test = model_.predict(X_test, num_iteration=LGBM.best_iteration_) # 예측\n",
    "            # If early stopping occurs, the model will have three additional fields: best_score, best_iteration and best_ntree_limit\n",
    "            # [eval_metric](https://stats.stackexchange.com/questions/493981/xgboost-mean-average-precision-eval-metric-for-classification)\n",
    "            # [gbm.fit verbose = 10, #verbose = 10 : 10번 반복할 때마다 logloss값을 보여준다.](https://dacon.io/codeshare/1827)\n",
    "            # [Classifier, Regressor eval_metric 예시](https://hwi-doc.tistory.com/entry/%EC%9D%B4%ED%95%B4%ED%95%98%EA%B3%A0-%EC%82%AC%EC%9A%A9%ED%95%98%EC%9E%90-XGBoost)\n",
    "            # [XGBoost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#)\n",
    "            # error: For MAP metric, there should be query information\n",
    "            elif model_nm == 'XGBoost': # xgboost_linear가 느려서\n",
    "                cntcnt += 1\n",
    "                print(f'$$$$$---{cntcnt}')\n",
    "                model_ = eval(model_nm).fit(X_train, y_train,\n",
    "                                            eval_set=[(X_test, y_test)],\n",
    "                                            eval_metric='auc',\n",
    "                                            early_stopping_rounds=5) # ,verbose = True\n",
    "                y_pred_test = model_.predict(X_test) # 예측\n",
    "            \n",
    "            else:\n",
    "                cntcnt += 1\n",
    "                print(f'$$$$$---{cntcnt}')\n",
    "                model_ = eval(model_nm).fit(X_train, y_train)\n",
    "                y_pred_test = model_.predict(X_test) # 예측\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred_test)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_test)\n",
    "#             f1 = f1_score(y_test, y_pred_test)\n",
    "#             jaccard = jaccard_score(y_test, y_pred_test)\n",
    "#             cv_scores = cross_val_score(eval(model_nm), tmpX, tmpy, cv=7, n_jobs = 64)\n",
    "            \n",
    "#             _ = dummy.fit(X_train, y_train)\n",
    "#             dummy_score = dummy.score(X_test, y_test)\n",
    "\n",
    "            results.append([i, f'{model_nm}', accuracy, roc_auc]) # , f1, jaccard, dummy_score, cv_scores\n",
    "            i+=1\n",
    "        ################################################################\n",
    "        print(results[-7:])\n",
    "    return results#score_dic#[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd65bcbc58295ddb1be3c2ed454fcbd526c999271179adb38fc6bed2214131e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
